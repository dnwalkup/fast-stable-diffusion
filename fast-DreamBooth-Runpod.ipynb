{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################################################# Dreambooth 4 Kaggle Mk2 ##############################################################\n",
    "#git\n",
    "# Based on u/Yacben's (https://github.com/TheLastBen/fast-stable-diffusion)\n",
    "# Adapated for Kaggle by u/shutonga (https://github.com/tuwonga/fast_Dreambooth_4_kaggle)\n",
    "# Mk2 by u/_rundown_\n",
    "#\n",
    "#\n",
    "# ** NOTE: Each of these code blocks must be run in order to complete the training of your model. **\n",
    "#\n",
    "# ** NOTE: Unless you know what you're doing, follow the guidance and you shouldn't have issues. **\n",
    "#\n",
    "#\n",
    "#\n",
    "#------------------------------------------------------------- UPLOAD TRAINING IMAGES -------------------------------------------------------------#\n",
    "# Let's add your training images.\n",
    "#\n",
    "#\n",
    "# Step 1 - Prepare your images\n",
    "#\n",
    "# ** NOTE: Skip to \"Step 2 - Upload your images\" if your images are already prepped.\n",
    "#\n",
    "# For training a person:\n",
    "# Use 10 or 20 images. 50% close-up/face, 30% medium/torso, 20% long/wide.\n",
    "#\n",
    "# For training a style:\n",
    "# Use 100 images. 70% people, 20% landscapes, 10% animals/objects. \n",
    "#\n",
    "# Sizing:\n",
    "# Make sure your images are PNG format. 512 x 512 is currently ideal.\n",
    "#\n",
    "# It is highly recommended to use a service like https://www.birme.net to resize and manually crop the focus area of your images, however, \n",
    "# the program below incldues an image cropping tool which will crop to the center of your images.\n",
    "#\n",
    "# Naming:\n",
    "# Rename the instance picture to the same instance unique identifier for each subject. This will be your trigger prompt in Stable Diffusion \n",
    "# to activate your person/style.\n",
    "#\n",
    "# Naming example:\n",
    "# If you have 20 pictures of yourself, simply select them all and rename only one to the chosen identifier: (e.g. phtmejhn)\n",
    "# Assuming you're on Windows, the files would resultantly be: phtmejhn (1).jpg, phtmejhn (2).png ... etc.\n",
    "#\n",
    "# Check out this example: https://i.imgur.com/d2lD3rz.jpeg\n",
    "#\n",
    "# Multiple subjects:\n",
    "# If you're training with multiple subjects (e.g. another 20 images of a different person) do the same renaming procedure \n",
    "# for other people or objects with their unique identifier.\n",
    "#\n",
    "#\n",
    "# Step 2 - Upload your images\n",
    "#\n",
    "#\n",
    "# ** NOTE: For more information, see Nitrosocke's great guide: https://github.com/nitrosocke/dreambooth-training-guide **\n",
    "#\n",
    "#\n",
    "#\n",
    "#---------------------------------------------------------------- GLOBAL VARIABLES ----------------------------------------------------------------#\n",
    "# You've got your images uploaded, you're settings are locked in, time to start this 'ish.\n",
    "#\n",
    "# Let's set some global variables that are important for the training of your model.\n",
    "#\n",
    "#\n",
    "# ** NOTE: Best not to use spaces.\n",
    "#\n",
    "#\n",
    "\n",
    "Huggingface_Token = \"\"\n",
    "# Downloading the Stable Diffusion model\n",
    "# ** NOTE: Leave EMPTY if you're using the v2 model **\n",
    "\n",
    "# To download the Stable Diffusion v1.5 checkpoint, a Hugging Face account is required and you must\n",
    "# accept the terms in https://huggingface.co/runwayml/stable-diffusion-v1-5\n",
    "\n",
    "# In your account settings > access tokens (https://huggingface.co/settings/tokens), you can create a new token. Then copy and paste it above\n",
    "# between the parenthesis.\n",
    "\n",
    "GPU_Select = \"RTX3090\"\n",
    "#If you know your GPU, enter it above. Current support includes \"A100\", \"V100\", \"P100\", \"T4\". \"RTX3090\" is available for CUDA11.4+Python2.8 on Linux.\n",
    "\n",
    "Model_Version = \"V2-768px\"\n",
    "# \"1.5\", \"V2-512px\", \"V2-768px\"\n",
    "\n",
    "Session_Name = \"ilya_768\"\n",
    "# Name your session. If it already exists, it will load it to continue.\n",
    "\n",
    "Training_Type = \"style\"\n",
    "# Current options are: person or style. Insert word in the quotes.\n",
    "\n",
    "Class_Token = True\n",
    "# Class token is a part of your activation prompt. True or False. If true, your class token will be the same as your Training Type.\n",
    "\n",
    "PT = \"ilyaart\"\n",
    "# Prompt token. This is what you'll add to your SD prompt to activate your style/person  \n",
    "\n",
    "Crop_images = False\n",
    "# Do you want the app to manually crop your images? True or False. Default is False (recommended you follow the above directions).\n",
    "\n",
    "Crop_size = \"768\"\n",
    "# Crop size is your ending image size. Advanced users can modify up to 1024. Default value is 512.\n",
    "\n",
    "Training_Steps = 5200\n",
    "# Number of instance images * 100 (e.g. if you use 30 images, use 3000 steps). Default value 1000.\n",
    "\n",
    "Learning_Rate_Def = 1e-6\n",
    "# Some have good results with 2e-6 and other options. Default value 1e-6.\n",
    "\n",
    "Save_Checkpoint_Every_n_Steps = True\n",
    "# Do you want to save checkpoints as you train? Useful if you want to train for over the recommended limit for best results.\n",
    "\n",
    "Save_Checkpoint_Every = 1300\n",
    "# Minimum 200 steps between each save.\n",
    "# ** NOTE: Remember you only have 20GB space on your Kaggle drive and each CKPT is > 2GB. **\n",
    "\n",
    "Start_saving_from_the_step = 1300\n",
    "# Step at which you would like to START saving (e.g. if this is 500, and Save_Checkpoint_Every is 1000, you will get a save at 500, 1500, 2500, etc.)\n",
    "Train_text_encoder_for = 100\n",
    "# This is the % of the total steps (Training_Steps variable) for which to train the Text Encoder.\n",
    "# If you're training a style, keep it between 10-20 (percent).\n",
    "# If you're training on a person, set it between 50-70 (percent). Reduce this if you can't stylize the person/object (overtrained).\n",
    "# Higher % will give more weight to the instance, it gives stronger results at lower steps count, but harder to stylize.\n",
    "\n",
    "GDrive_Inst_Images = \"https://drive.google.com/file/d/1cK0gV8WtByP2Hr39F0mRf2176sb8pHi5/view?usp=sharing\"\n",
    "# Link to your images on your GDrive\n",
    "\n",
    "\n",
    "#-------------------------------------------------------- GLOBAL VARIABLES - ERROR OPTIONS ---------------------------------------------------------#\n",
    " \n",
    "Reduce_memory_usage = True\n",
    "# If you're getting memory issues, change this to True (slower speed but memory effecient)\n",
    "\n",
    "Compatibility_Mode = \"\"\n",
    "# Enable only if you're getting model conversion errors for advanced custom CKPTs. True or False.\n",
    "\n",
    "\n",
    "#----------------------------------------------------- GLOBAL VARIABLES - ADVANCED OPTIONS --------------------------------------------------------#\n",
    "#\n",
    "# ** NOTE: If you're unsure about these settings, leave them as-is.\n",
    "\n",
    "fp16 = True\n",
    "# Enable/disable half-precision, disabling it will double the training time and produce 4.7Gb checkpoints.\n",
    "\n",
    "Seed = ''\n",
    "# Leave empty for a random seed.\n",
    "\n",
    "Captionned_instance_images = True\n",
    "# Bool - Learn the caption from your images?\n",
    "\n",
    "Session_Link_optional = \"\"\n",
    "# Import a session from another gdrive\n",
    "# The shared gdrive link must point to the specific session's folder that contains the trained CKPT, remove the intermediary CKPT if any exist.\n",
    "\n",
    "Resume_Training = False\n",
    "# If starting from scratch, this should be False. If you're resuming a training, True.\n",
    "\n",
    "Enable_text_encoder_training = True\n",
    "# If planning to resume training, you must have a total of 10% text encoder training. This can be at the beginning, middle, or end.\n",
    "# For example, you can plan to distribute the text encoder at 15% equally over 3 training sessions in 5%, 5%, 5%, \n",
    "# Or 0%, 0%, 15%, given that 15% will cover the total training steps count (15% of 200 steps is not enough).\n",
    "\n",
    "\n",
    "# Options for using a different model:\n",
    "\n",
    "Path_to_HuggingFace= \"\" #Use the format \"profile/model\" (e.g. runwayml/stable-diffusion-v1-5)\n",
    "\n",
    "CKPT_Path = \"\" #Kaggle path to an uploaded CKPT\n",
    "\n",
    "CKPT_Link = \"\" #A CKPT direct link, huggingface CKPT link, or a shared CKPT from gdrive.\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "#-------------------------------------------------------------------- IMPORTS ---------------------------------------------------------------------#\n",
    "# Now for the code. Don't change anything here:\n",
    "#\n",
    "\n",
    "import os, sys, gc, time, shutil, random, wget, ftfy\n",
    "\n",
    "from subprocess import getoutput\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "from IPython.utils import capture\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "from IPython.display import Javascript\n",
    "from IPython.display import FileLink\n",
    "\n",
    "#!export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu\n",
    "#export VARIABLE=value\n",
    "\n",
    "clear_output()\n",
    "print('\u001b[1;32mInitial setup complete, variables set, please move on to the next cell !')\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "# ** NOTE: YOU MUST RUN THIS CELL FOR THE PROGRAM TO EXECUTE AND REMEMBER YOUR SETTINGS! **\n",
    "#\n",
    "#\n",
    "####################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build content directories\n",
    "\n",
    "%cd /\n",
    "!mkdir /workspace/content\n",
    "!mkdir /workspace/content/gdrive\n",
    "!mkdir /workspace/content/gdrive/MyDrive\n",
    "!mkdir /workspace/content/gdrive/MyDrive/Fast-Dreambooth\n",
    "!mkdir /workspace/content/models\n",
    "!mkdir /workspace/content/gdrive/MyDrive/Fast-Dreambooth/Sessions\n",
    "\n",
    "%cd /workspace/content\n",
    "\n",
    "clear_output()\n",
    "print('\u001b[1;32mContent dirs created, please move on to the next cell !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Diffusers and dependencies\n",
    "\n",
    "%cd /workspace/content\n",
    "\n",
    "!apt update\n",
    "!apt-get -y install git\n",
    "!apt-get -y install git-lfs\n",
    "!git lfs install\n",
    "\n",
    "!git clone --branch updt https://github.com/TheLastBen/diffusers\n",
    "!pip install git+https://github.com/TheLastBen/diffusers@updt\n",
    "!pip install open_clip_torch\n",
    "!pip install torchsde\n",
    "!pip install pytorch_lightning\n",
    "!pip install huggingface_hub\n",
    "!pip install -U --no-cache-dir gdown\n",
    "!pip install accelerate\n",
    "!pip install bitsandbytes\n",
    "!pip install --upgrade requests\n",
    "!pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "\n",
    "clear_output()\n",
    "print('\u001b[1;32mDependencies installed, please move on to the next cell !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install xformers\n",
    "\n",
    "if (GPU_Select == \"T4\"):\n",
    "  %pip install https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "  clear_output()\n",
    "  print('\u001b[1;32mInstalled xformers for your selected GPU to speed up training your model, please move on to the next cell !')\n",
    "  \n",
    "elif (GPU_Select == \"P100\"):\n",
    "  %pip install https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "  clear_output()\n",
    "  print('\u001b[1;32mInstalled xformers for your selected GPU to speed up training your model, please move on to the next cell !')\n",
    "\n",
    "elif (GPU_Select == \"V100\"):\n",
    "  %pip install https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "  clear_output()\n",
    "  print('\u001b[1;32mInstalled xformers for your selected GPU to speed up training your model, please move on to the next cell !')\n",
    "\n",
    "elif (GPU_Select == \"A100\"):\n",
    "  %pip install https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "  clear_output()\n",
    "  print('\u001b[1;32mInstalled xformers for your selected GPU to speed up training your model, please move on to the next cell !')\n",
    "\n",
    "elif (GPU_Select == \"RTX3090\"):\n",
    "  %pip install https://huggingface.co/dnwalkup/xformers-precompiles/resolve/main/RTX3090/xformers-0.0.15.dev0%2Bc733c99.d20221129-cp38-cp38-linux_x86_64.whl\n",
    "  #clear_output()\n",
    "  print('\u001b[1;32mInstalled xformers for your selected GPU to speed up training your model, please move on to the next cell !')\n",
    "\n",
    "else:\n",
    "    print('\u001b[1;31mit seems that your GPU is not supported at the moment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell downloads the Stable Diffusion checkpoint to your working folder.\n",
    "#\n",
    "# ** NOTE: Skip this cell if you are loading a previous session (you don't need to download it more than once)\n",
    "#\n",
    "\n",
    "#@markdown ---\n",
    "\n",
    "with capture.capture_output() as cap: \n",
    "  %cd /workspace/content/\n",
    "\n",
    "if Model_Version == \"V2-512px\" or Model_Version == \"V2-768px\":\n",
    "  Huggingface_Token = \"\"\n",
    "\n",
    "token = Huggingface_Token\n",
    "\n",
    "\n",
    "def downloadmodel():\n",
    "  \n",
    "  token = Huggingface_Token\n",
    "  \n",
    "  if os.path.exists('/workspace/content/stable-diffusion-v1-5'):\n",
    "    !rm -r /workspace/content/stable-diffusion-v1-5\n",
    "\n",
    "  %cd /workspace/content/\n",
    "  !mkdir /workspace/content/stable-diffusion-v1-5\n",
    "  %cd /workspace/content/stable-diffusion-v1-5\n",
    "\n",
    "  !git init\n",
    "  !git lfs install --system --skip-repo\n",
    "  !git remote add -f origin  \"https://USER:{token}@huggingface.co/runwayml/stable-diffusion-v1-5\"\n",
    "  !git config core.sparsecheckout true\n",
    "  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nmodel_index.json\" > .git/info/sparse-checkout\n",
    "  !git pull origin main\n",
    "\n",
    "  if os.path.exists('/workspace/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
    "    !git clone \"https://USER:{token}@huggingface.co/stabilityai/sd-vae-ft-mse\"\n",
    "    !mv /workspace/content/stable-diffusion-v1-5/sd-vae-ft-mse /workspace/content/stable-diffusion-v1-5/vae\n",
    "    !rm -r /workspace/content/stable-diffusion-v1-5/.git\n",
    "    %cd /workspace/content/stable-diffusion-v1-5    \n",
    "    !rm model_index.json\n",
    "    time.sleep(1)    \n",
    "    wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/model_index.json')\n",
    "    !sed -i 's@\"clip_sample\": false@@g' /workspace/content/stable-diffusion-v1-5/scheduler/scheduler_config.json\n",
    "    !sed -i 's@\"trained_betas\": null,@\"trained_betas\": null@g' /workspace/content/stable-diffusion-v1-5/scheduler/scheduler_config.json\n",
    "    !sed -i 's@\"sample_size\": 256,@\"sample_size\": 512,@g' /workspace/content/stable-diffusion-v1-5/vae/config.json  \n",
    "    %cd /workspace/content/    \n",
    "    \n",
    "    clear_output()\n",
    "    print('\u001b[1;32mDownloaded SDv1.5 from RunwayML on Hugging Face, please move on to the next cell !')\n",
    "  \n",
    "  else:\n",
    "    while not os.path.exists('/workspace/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
    "         print('\u001b[1;31mMake sure you accepted the terms in https://huggingface.co/runwayml/stable-diffusion-v1-5')\n",
    "         time.sleep(5)\n",
    "\n",
    "        \n",
    "def newdownloadmodel():\n",
    "\n",
    "  %cd /workspace/content/\n",
    "\n",
    "  # Function to download SDv2 from Hugging Face\n",
    "  '''\n",
    "  !mkdir /workspace/content/stable-diffusion-v2-768\n",
    "  %cd /workspace/content/stable-diffusion-v2-768\n",
    "  \n",
    "  !git init\n",
    "  !git lfs install --system --skip-repo\n",
    "  !git remote add -f origin  \"https://USER:{token}@huggingface.co/stabilityai/stable-diffusion-2\"\n",
    "  !git config core.sparsecheckout true\n",
    "  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nmodel_index.json\" > .git/info/sparse-checkout\n",
    "  !git pull origin main\n",
    "\n",
    "  %cd /workspace/content/stable-diffusion-v2-768\n",
    "  !mkdir scheduler\n",
    "  %cd scheduler\n",
    "  !wget https://huggingface.co/stabilityai/stable-diffusion-2/raw/main/scheduler/scheduler_config.json\n",
    "  '''\n",
    "\n",
    "  # GDrive function for faster speeds\n",
    "  !gdown --fuzzy https://drive.google.com/file/d/1NnpSM971X5vAbGgvFMaMbKLvqAfEUSt8/view?usp=sharing -O sdv2.zip\n",
    "  !unzip sdv2.zip\n",
    "  !rm sdv2.zip\n",
    "  !mv /workspace/content/stable-diffusion-2 /workspace/content/stable-diffusion-v2-768\n",
    "\n",
    "  clear_output()\n",
    "  print('\u001b[1;32mDownloaded SD V2 768 model from StabilityAI on Hugging Face, please move on to the next cell !')\n",
    "\n",
    "\n",
    "def newdownloadmodelb():\n",
    "\n",
    "  %cd /workspace/content/\n",
    "\n",
    "  !mkdir /workspace/content/stable-diffusion-v2-512\n",
    "  %cd /workspace/content/stable-diffusion-v2-512\n",
    "\n",
    "  !git init\n",
    "  !git lfs install --system --skip-repo\n",
    "  !git remote add -f origin  \"https://USER:{token}@huggingface.co/stabilityai/stable-diffusion-2-base\"\n",
    "  !git config core.sparsecheckout true\n",
    "  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nvae\\nmodel_index.json\" > .git/info/sparse-checkout\n",
    "  !git pull origin main\n",
    "\n",
    "  clear_output()\n",
    "  print('\u001b[1;32mDownloaded SD V2 512 model from StabilityAI on Hugging Face, please move on to the next cell !')\n",
    "\n",
    "        \n",
    "if Path_to_HuggingFace != \"\":\n",
    "  \n",
    "  if os.path.exists('/workspace/content/stable-diffusion-custom'):\n",
    "    !rm -r /contkaggle/working/contentent/stable-diffusion-custom\n",
    "  clear_output()\n",
    "\n",
    "  %cd /workspace/content/\n",
    "  clear_output()\n",
    "\n",
    "  !mkdir /workspace/content/stable-diffusion-custom\n",
    "  %cd /workspace/content/stable-diffusion-custom\n",
    "\n",
    "  !git init\n",
    "  !git lfs install --system --skip-repo\n",
    "  !git remote add -f origin  \"https://USER:{token}@huggingface.co/{Path_to_HuggingFace}\"\n",
    "  !git config core.sparsecheckout true\n",
    "  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nmodel_index.json\" > .git/info/sparse-checkout\n",
    "  !git pull origin main\n",
    "\n",
    "  if os.path.exists('/workspace/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n",
    "    !git clone \"https://USER:{token}@huggingface.co/stabilityai/sd-vae-ft-mse\"\n",
    "    !mv /workspace/content/stable-diffusion-custom/sd-vae-ft-mse /workspace/content/stable-diffusion-custom/vae\n",
    "    !rm -r /workspace/content/stable-diffusion-custom/.git\n",
    "    %cd /workspace/content/stable-diffusion-custom\n",
    "    !rm model_index.json\n",
    "    time.sleep(1)\n",
    "    wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/model_index.json')\n",
    "    !sed -i 's@\"clip_sample\": false@@g' /workspace/content/stable-diffusion-custom/scheduler/scheduler_config.json\n",
    "    !sed -i 's@\"trained_betas\": null,@\"trained_betas\": null@g' /workspace/content/stable-diffusion-custom/scheduler/scheduler_config.json\n",
    "    !sed -i 's@\"sample_size\": 256,@\"sample_size\": 512,@g' /workspace/content/stable-diffusion-custom/vae/config.json    \n",
    "    %cd /workspace/content/ \n",
    "    MODEL_NAME = \"/workspace/content/stable-diffusion-custom\"\n",
    "\n",
    "    clear_output()\n",
    "    print('\u001b[1;32mDownloaded your custom model from Hugging Face, please move on to the next cell !')\n",
    "\n",
    "  else:\n",
    "    while not os.path.exists('/workspace/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n",
    "          print('\u001b[1;32mCheck the link you provided')\n",
    "          time.sleep(5)\n",
    "\n",
    "\n",
    "elif CKPT_Path != \"\":\n",
    "\n",
    "  if os.path.exists('/workspace/content/stable-custom'):\n",
    "    !rm -r /workspace/content/stable-diffusion-custom\n",
    "  \n",
    "  if os.path.exists(str(CKPT_Path)):\n",
    "    !mkdir /workspace/content/stable-diffusion-custom\n",
    "    \n",
    "    with capture.capture_output() as cap:\n",
    "      if Compatibility_Mode:\n",
    "        !wget https://raw.githubusercontent.com/huggingface/diffusers/039958eae55ff0700cfb42a7e72739575ab341f1/scripts/convert_original_stable_diffusion_to_diffusers.py\n",
    "        !python /workspace/content/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path \"$CKPT_Path\" --dump_path /workspace/content/stable-diffusion-custom\n",
    "        !rm /workspace/content/convert_original_stable_diffusion_to_diffusers.py\n",
    "      else:           \n",
    "        !python /workspace/content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path \"$CKPT_Path\" --dump_path /workspace/content/stable-diffusion-custom\n",
    "    \n",
    "    if os.path.exists('/conkaggle/working/contenttent/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n",
    "      !rm /workspace/content/v1-inference.yaml\n",
    "      clear_output()\n",
    "      MODEL_NAME = \"/workspace/content/stable-diffusion-custom\"\n",
    "      print('\u001b[1;32mCheckpoint converted from custom path, please move on to the next cell !')\n",
    "    else:\n",
    "      !rm /workspace/content/convert_original_stable_diffusion_to_diffusers.py\n",
    "      !rm /workspace/content/v1-inference.yaml\n",
    "      !rm -r /workspace/content/stable-diffusion-custom\n",
    "      while not os.path.exists('/workspace/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n",
    "        print('\u001b[1;32mConversion error, Insufficient RAM or corrupt CKPT, use a 4GB CKPT instead of 7GB')\n",
    "        time.sleep(5)\n",
    "  \n",
    "  else:\n",
    "    while not os.path.exists(str(CKPT_Path)):\n",
    "       print('\u001b[1;32mWrong path, use the file explorer to copy the path')\n",
    "       time.sleep(5)\n",
    "\n",
    "\n",
    "elif CKPT_Link != \"\":   \n",
    "    \n",
    "    if os.path.exists('/workspace/content/stable-diffusion-custom'):\n",
    "      !rm -r /workspace/content/stable-diffusion-custom   \n",
    "    \n",
    "    !gdown --fuzzy $CKPT_Link -O model.ckpt    \n",
    "    \n",
    "    if os.path.exists('/workspace/content/model.ckpt'):\n",
    "      if os.path.getsize(\"/workspace/content/model.ckpt\") > 1810671599:\n",
    "        !mkdir /workspace/content/stable-diffusion-custom\n",
    "        \n",
    "        with capture.capture_output() as cap: \n",
    "          if Compatibility_Mode:\n",
    "            !wget https://raw.githubusercontent.com/huggingface/diffusers/039958eae55ff0700cfb42a7e72739575ab341f1/scripts/convert_original_stable_diffusion_to_diffusers.py\n",
    "            !python /workspace/content/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path /workspace/content/model.ckpt --dump_path /workspace/content/stable-diffusion-custom\n",
    "            !rm /workspace/content/convert_original_stable_diffusion_to_diffusers.py            \n",
    "          else:           \n",
    "            !python /workspace/content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path /workspace/content/model.ckpt --dump_path /workspace/content/stable-diffusion-custom\n",
    "        \n",
    "        if os.path.exists('/workspace/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n",
    "          clear_output()\n",
    "          MODEL_NAME = \"/workspace/content/stable-diffusion-custom\"\n",
    "          print('\u001b[1;32mCheckpoint converted from custom path, please move on to the next cell !')\n",
    "          !rm /workspace/content/v1-inference.yaml\n",
    "          !rm /workspace/content/model.ckpt\n",
    "        \n",
    "        else:\n",
    "          if os.path.exists('/workspace/content/v1-inference.yaml'):\n",
    "            !rm /workspace/content/v1-inference.yaml\n",
    "          !rm /workspace/content/convert_original_stable_diffusion_to_diffusers.py\n",
    "          !rm -r /workspace/content/stable-diffusion-custom\n",
    "          !rm /workspace/content/model.ckpt\n",
    "          while not os.path.exists('/workspace/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n",
    "            print('\u001b[1;32mConversion error, Insufficient RAM or corrupt CKPT, use a 4GB CKPT instead of 7GB')\n",
    "            time.sleep(5)\n",
    "      \n",
    "      else:\n",
    "        while os.path.getsize('/workspace/content/model.ckpt') < 1810671599:\n",
    "           print('\u001b[1;32mWrong link, check that the link is valid')\n",
    "           time.sleep(5)\n",
    "\n",
    "\n",
    "else:\n",
    "  \n",
    "  if Model_Version == \"V2-512px\":\n",
    "    if not os.path.exists('/workspace/content/stable-diffusion-v2-512'):\n",
    "      newdownloadmodelb()\n",
    "      MODEL_NAME=\"/workspace/content/stable-diffusion-v2-512\"\n",
    "    else:\n",
    "      print(\"\u001b[1;32mThe v2-512px model already exist, using this model.\")      \n",
    "  \n",
    "  elif Model_Version == \"V2-768px\":\n",
    "    if not os.path.exists('/workspace/content/stable-diffusion-v2-768'):   \n",
    "      newdownloadmodel()\n",
    "      MODEL_NAME = \"/workspace/content/stable-diffusion-v2-768\"\n",
    "    else:\n",
    "      print(\"\u001b[1;32mThe v2-768px model already exist, using this model.\")\n",
    "\n",
    "  else:\n",
    "    if not os.path.exists('/workspace/content/stable-diffusion-v1-5'):\n",
    "      downloadmodel()\n",
    "      MODEL_NAME = \"/workspace/content/stable-diffusion-v1-5\"\n",
    "    else:\n",
    "      print(\"\u001b[1;32mThe v1.5 model already exist, using this model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create or load a session\n",
    "\n",
    "try:\n",
    "  MODEL_NAME\n",
    "except:\n",
    "  MODEL_NAME = \"\"\n",
    "  print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n",
    "\n",
    "\n",
    "if PT != \"\":\n",
    "  Captionned_instance_images = False\n",
    "\n",
    "\n",
    "Session_Name = Session_Name.replace(\" \",\"_\")\n",
    "\n",
    "WORKSPACE = '/workspace/content/gdrive/MyDrive/Fast-Dreambooth'\n",
    "\n",
    "\n",
    "if Session_Link_optional != \"\":\n",
    "  print('\u001b[1;32mDownloading session...')\n",
    "\n",
    "with capture.capture_output() as cap:\n",
    "  %cd /workspace/content\n",
    "  \n",
    "  if Session_Link_optional != \"\":\n",
    "    if not os.path.exists(str(WORKSPACE+'/Sessions')):\n",
    "      %mkdir -p $WORKSPACE'/Sessions'\n",
    "      time.sleep(1)\n",
    "    %cd $WORKSPACE'/Sessions'\n",
    "    !gdown --folder --remaining-ok -O $Session_Name $Session_Link_optional\n",
    "    %cd $Session_Name\n",
    "    !rm -r instance_images\n",
    "    !rm -r Regularization_images\n",
    "    !unzip instance_images.zip\n",
    "    !mv *.ckpt $Session_Name\".ckpt\"\n",
    "    %cd /workspace/content\n",
    "\n",
    "    \n",
    "INSTANCE_NAME = Session_Name\n",
    "OUTPUT_DIR = \"/workspace/content/models/\" + Session_Name\n",
    "SESSION_DIR = WORKSPACE + '/Sessions/' + Session_Name\n",
    "INSTANCE_DIR = SESSION_DIR + '/instance_images'\n",
    "MDLPTH = str(SESSION_DIR+\"/\"+Session_Name+'.ckpt')\n",
    "CLASS_DIR = SESSION_DIR + '/Regularization_images'\n",
    "\n",
    "\n",
    "if os.path.exists(str(SESSION_DIR)):\n",
    "  if not os.path.exists(MDLPTH) and '.ckpt' in str([ckpt for ckpt in listdir(SESSION_DIR) if ckpt.split(\".\")[-1]==\"ckpt\"]):  \n",
    "    print('\u001b[1;32mSkipping the intermediary checkpoints.')\n",
    "\n",
    "    \n",
    "if os.path.exists(str(SESSION_DIR)) and not os.path.exists(MDLPTH):\n",
    "  print('\u001b[1;32mLoading session with no previous model, using the original model or the custom downloaded model')\n",
    "  if MODEL_NAME == \"\":\n",
    "    print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n",
    "  else:\n",
    "    print('\u001b[1;32mSession Loaded, proceed to uploading instance images')\n",
    "\n",
    "\n",
    "elif os.path.exists(MDLPTH):\n",
    "  print('\u001b[1;32mSession found, loading the trained model ...')\n",
    "  %mkdir -p \"$OUTPUT_DIR\"\n",
    "  !python /workspace/content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path \"$MDLPTH\" --dump_path \"$OUTPUT_DIR\" --session_dir \"$SESSION_DIR\"\n",
    "  if os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n",
    "    resume = True    \n",
    "    !rm /workspace/content/v1-inference.yaml\n",
    "    clear_output()\n",
    "    print('\u001b[1;32mSession loaded.')\n",
    "  else:     \n",
    "    !rm /workspace/content/v1-inference.yaml\n",
    "    if not os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n",
    "      print('\u001b[1;31mConversion error, if the error persists, remove the CKPT file from the current session folder')\n",
    "\n",
    "\n",
    "elif not os.path.exists(str(SESSION_DIR)):\n",
    "    %mkdir -p \"$INSTANCE_DIR\"\n",
    "    print('\u001b[1;32mCreating session...')\n",
    "    if MODEL_NAME == \"\":\n",
    "      print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n",
    "    else:\n",
    "      print('\u001b[1;32mSession created, proceed to uploading instance images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images from Google Drive\n",
    "\n",
    "!mkdir /workspace/my\n",
    "%cd /workspace/my\n",
    "\n",
    "!rm -r /workspace/my/*\n",
    "!gdown --fuzzy $GDrive_Inst_Images -O user_images.zip\n",
    "!unzip user_images.zip\n",
    "!rm user_images.zip\n",
    "\n",
    "for instance_items in os.scandir(os.getcwd()):\n",
    "    if instance_items.is_dir():\n",
    "        INSTANCE_DIR_TMP = instance_items.path\n",
    "\n",
    "os.rename(INSTANCE_DIR_TMP,\"user_images\")\n",
    "\n",
    "%cd /workspace/my/user_images\n",
    "!find . -name \"* *\" -type f | rename 's/ /_/g'\n",
    "\n",
    "del INSTANCE_DIR_TMP\n",
    "gc.collect()\n",
    "\n",
    "clear_output()\n",
    "print('\u001b[1;32mInstance images copied to working directory, please move on to the next cell !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download and initialize regularization images \n",
    "\n",
    "if Model_Version == \"V2-768px\":\n",
    "    if Training_Type.lower() == \"person\":\n",
    "        Num_Class_Images_Def = 1000\n",
    "        %cd $SESSION_DIR\n",
    "        !rm -r $SESSION_DIR/*\n",
    "        !gdown --fuzzy https://drive.google.com/file/d/1--UP6J_JlixzAKYF8x327RidnsRphAvK/view?usp=share_link -O Regz.zip\n",
    "        !unzip Regz.zip\n",
    "        !rm Regz.zip\n",
    "    else:\n",
    "        Num_Class_Images_Def = 1000\n",
    "        %cd $SESSION_DIR\n",
    "        !rm -r $SESSION_DIR/*\n",
    "        !gdown --fuzzy https://drive.google.com/file/d/1zZXLMQ-Uzpsv-NCsvvETtaeZhaYnTxBW/view?usp=sharing -O Regz.zip\n",
    "        !unzip Regz.zip\n",
    "        !rm Regz.zip\n",
    "\n",
    "else:\n",
    "    if Training_Type.lower() == \"person\":\n",
    "        Num_Class_Images_Def = 1000\n",
    "        %cd $SESSION_DIR\n",
    "        !rm -r $SESSION_DIR/*\n",
    "        !gdown --fuzzy https://drive.google.com/file/d/1dsfY9SF7992t7cc8O-DY1UYCW6Jn6hKz/view?usp=share_link -O Regz.zip\n",
    "        !unzip Regz.zip\n",
    "        !rm Regz.zip\n",
    "    else:\n",
    "        Num_Class_Images_Def = 1001\n",
    "        %cd $SESSION_DIR\n",
    "        !rm -r $SESSION_DIR/*\n",
    "        !gdown --fuzzy https://drive.google.com/file/d/1d0KsluHx-ZaYCGThxZMuTxVbor2pROlF/view?usp=share_link -O Regz.zip\n",
    "        !unzip Regz.zip\n",
    "        !rm Regz.zip\n",
    "\n",
    "for reg_items in os.scandir(os.getcwd()):\n",
    "    if reg_items.is_dir():\n",
    "        REG_DIR = reg_items.path\n",
    "\n",
    "os.rename(REG_DIR,\"reg_images\")\n",
    "\n",
    "CLASS_DIR = SESSION_DIR + '/reg_images'\n",
    "\n",
    "os.chdir(CLASS_DIR)\n",
    "!find . -name \"* *\" -type f | rename 's/ /_/g'\n",
    "\n",
    "del REG_DIR\n",
    "gc.collect()\n",
    "\n",
    "clear_output()\n",
    "print('\u001b[1;32mRegularization images initialized, please move on to the next cell !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance images modifications including crop.\n",
    "#\n",
    "# ** NOTE: EVEN IF NOT CROPPING, THIS CELL MUST BE RUN **\n",
    "\n",
    "%cd /workspace/content\n",
    "\n",
    "Remove_existing_instance_images = True #@param{type: 'boolean'}\n",
    "#@markdown - Uncheck the box to keep the existing instance images.\n",
    "\n",
    "if Remove_existing_instance_images:\n",
    "  if os.path.exists(str(INSTANCE_DIR)):\n",
    "    !rm -r \"$INSTANCE_DIR\"\n",
    "\n",
    "if not os.path.exists(str(INSTANCE_DIR)):\n",
    "  %mkdir -p \"$INSTANCE_DIR\"\n",
    "\n",
    "IMAGES_FOLDER_OPTIONAL = \"/workspace/my\" #@param{type: 'string'} this is the working directory for your Kaggle instance images. DO NOT CHANGE.\n",
    "\n",
    "#@markdown - Crop script\n",
    "\n",
    "Crop_size = int(Crop_size)\n",
    "\n",
    "if IMAGES_FOLDER_OPTIONAL != \"\":\n",
    "  if Crop_images:\n",
    "    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n",
    "      extension = filename.split(\".\")[1]\n",
    "      identifier = filename.split(\".\")[0]\n",
    "      new_path_with_file = os.path.join(INSTANCE_DIR, filename)\n",
    "      file = Image.open(IMAGES_FOLDER_OPTIONAL+\"/\"+filename)\n",
    "      width, height = file.size\n",
    "      if file.size != (Crop_size, Crop_size):      \n",
    "        side_length = min(width, height)\n",
    "        left = (width - side_length)/2\n",
    "        top = (height - side_length)/2\n",
    "        right = (width + side_length)/2\n",
    "        bottom = (height + side_length)/2\n",
    "        image = file.crop((left, top, right, bottom))\n",
    "        image = image.resize((Crop_size, Crop_size))\n",
    "        image.save(new_path_with_file, format=\"PNG\", quality = 100)\n",
    "      else:\n",
    "        !cp \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n",
    "\n",
    "  else:\n",
    "    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n",
    "      %cp -r \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n",
    " \n",
    "  print('\\n\u001b[1;32mComplete!')\n",
    "\n",
    "\n",
    "elif IMAGES_FOLDER_OPTIONAL == \"\":\n",
    "  uploaded = files.upload()\n",
    "  if Crop_images:\n",
    "    for filename in tqdm(uploaded.keys(), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n",
    "      shutil.move(filename, INSTANCE_DIR)\n",
    "      extension = filename.split(\".\")[1]\n",
    "      identifier = filename.split(\".\")[0]\n",
    "      new_path_with_file = os.path.join(INSTANCE_DIR, filename)\n",
    "      file = Image.open(new_path_with_file)\n",
    "      width, height = file.size\n",
    "      if file.size != (Crop_size, Crop_size):        \n",
    "        side_length = min(width, height)\n",
    "        left = (width - side_length)/2\n",
    "        top = (height - side_length)/2\n",
    "        right = (width + side_length)/2\n",
    "        bottom = (height + side_length)/2\n",
    "        image = file.crop((left, top, right, bottom))\n",
    "        image = image.resize((Crop_size, Crop_size))\n",
    "        image.save(new_path_with_file, format=\"PNG\", quality = 100)\n",
    "\n",
    "      else:\n",
    "          image.save(new_path_with_file, format=extension.upper())\n",
    "      clear_output()\n",
    "  else:\n",
    "    for filename in tqdm(uploaded.keys(), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n",
    "      shutil.move(filename, INSTANCE_DIR)\n",
    "      clear_output()\n",
    "\n",
    "  print('\\n\u001b[1;32mDone!')\n",
    "\n",
    "with capture.capture_output() as cap:\n",
    "  %cd \"$INSTANCE_DIR\"\n",
    "  !find . -name \"* *\" -type f | rename 's/ /-/g'\n",
    "  %cd /content\n",
    "  if os.path.exists(INSTANCE_DIR+\"/.ipynb_checkpoints\"):\n",
    "    %rm -r INSTANCE_DIR+\"/.ipynb_checkpoints\"    \n",
    "\n",
    "  %cd $SESSION_DIR\n",
    "  !rm instance_images.zip\n",
    "  !zip -r instance_images instance_images\n",
    "\n",
    "%cd $INSTANCE_DIR\n",
    "\n",
    "for all_dirs in os.scandir(os.getcwd()):\n",
    "    if all_dirs.is_dir():\n",
    "        INSTANCE_DIR = all_dirs.path\n",
    "        \n",
    "del all_dirs\n",
    "gc.collect()\n",
    "\n",
    "clear_output()\n",
    "print('\u001b[1;32mDone, time to start training ! Move on to the next cell !')\n",
    "\n",
    "print(INSTANCE_DIR)\n",
    "\n",
    "%cd /workspace/content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START DREAMBOOTH TRAINING !!\n",
    "\n",
    "while not Resume_Training and MODEL_NAME == \"\":\n",
    "  print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n",
    "  time.sleep(5)\n",
    "\n",
    "\n",
    "MODELT_NAME = MODEL_NAME\n",
    "\n",
    "Resolution = Crop_size\n",
    "\n",
    "Res_Int = int(Resolution)\n",
    "\n",
    "GC = \"\"\n",
    "if Reduce_memory_usage or Resolution != \"512\":\n",
    "  GC = \"--gradient_checkpointing\"\n",
    "\n",
    "if Seed == '' or Seed == '0':\n",
    "  Seed = random.randint(1, 999999)\n",
    "else:\n",
    "  Seed = int(Seed)\n",
    "\n",
    "if fp16:\n",
    "  prec = \"fp16\"\n",
    "else:\n",
    "  prec = \"no\"\n",
    "\n",
    "precision = prec\n",
    "\n",
    "\n",
    "CT = \"\"\n",
    "ClassPromptVar = ''\n",
    "\n",
    "if Class_Token:\n",
    "  CT = Training_Type\n",
    "  ClassPromptVar = '--class_prompt=\"$CT\"'\n",
    "\n",
    "\n",
    "Caption = ''\n",
    "\n",
    "if Captionned_instance_images:\n",
    "  Caption = '--image_captions_filename'\n",
    "  \n",
    "\n",
    "if Resume_Training and os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n",
    "  MODELT_NAME = OUTPUT_DIR\n",
    "  print('\u001b[1;32mResuming Training...\u001b[0m')\n",
    "\n",
    "elif Resume_Training and not os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n",
    "  print('\u001b[1;31mPrevious model not found, training a new model...\u001b[0m') \n",
    "  MODELT_NAME = MODEL_NAME\n",
    "  while MODEL_NAME == \"\":\n",
    "    print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n",
    "    time.sleep(5)\n",
    "\n",
    "    \n",
    "#@markdown **************************************************** TEXT ENCODER ****************************************************\n",
    "\n",
    "if Train_text_encoder_for >= 100:\n",
    "  stptxt = Training_Steps\n",
    "elif Train_text_encoder_for == 0:\n",
    "  Enable_text_encoder_training = False\n",
    "  stptxt = 10\n",
    "else:\n",
    "  stptxt = int((Training_Steps*Train_text_encoder_for)/100)\n",
    "\n",
    "if Enable_text_encoder_training:\n",
    "  Textenc=\"--train_text_encoder\"\n",
    "else:\n",
    "  Textenc=\"\"\n",
    "\n",
    "\n",
    "#@markdown **************************************************** CKPT SAVES ***********************************************************\n",
    "\n",
    "# ****************** Dreambooth absolute path fix **********************\n",
    "!chmod -R 755 /workspace/content/diffusers/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
    "\n",
    "%cd /workspace/content/diffusers/examples/dreambooth/\n",
    "\n",
    "search_text_pathfix = \"python /content\"\n",
    "replace_text_pathfix = \"python3 /workspace/content\"\n",
    "\n",
    "with open(r'train_dreambooth.py', 'r') as file_pathfix:\n",
    "  data_pathfix = file_pathfix.read()\n",
    "  data_pathfix = data_pathfix.replace(search_text_pathfix, replace_text_pathfix)\n",
    "\n",
    "with open(r'train_dreambooth.py', 'w') as file_pathfix:\n",
    "  file_pathfix.write(data_pathfix)\n",
    "\n",
    "search_text_pathfix = \"+inst+\"\n",
    "replace_text_pathfix = \"+ckpt_name+\"\n",
    "\n",
    "with open(r'train_dreambooth.py', 'r') as file_pathfix:\n",
    "  data_pathfix = file_pathfix.read()\n",
    "  data_pathfix = data_pathfix.replace(search_text_pathfix, replace_text_pathfix)\n",
    "\n",
    "with open(r'train_dreambooth.py', 'w') as file_pathfix:\n",
    "  file_pathfix.write(data_pathfix)\n",
    "\n",
    "del search_text_pathfix\n",
    "del replace_text_pathfix\n",
    "del file_pathfix\n",
    "del data_pathfix\n",
    "gc.collect()\n",
    "\n",
    "%cd /workspace/content/\n",
    "\n",
    "\n",
    "# ****************** Back to CKPT saves **********************\n",
    "if Save_Checkpoint_Every == None:\n",
    "  Save_Checkpoint_Every = 1\n",
    "\n",
    "stp = 0\n",
    "\n",
    "if Start_saving_from_the_step == None:\n",
    "  Start_saving_from_the_step = 0\n",
    "\n",
    "if (Start_saving_from_the_step < 200):\n",
    "  Start_saving_from_the_step = Save_Checkpoint_Every\n",
    "\n",
    "stpsv = Start_saving_from_the_step\n",
    "\n",
    "if Save_Checkpoint_Every_n_Steps:\n",
    "  stp = Save_Checkpoint_Every\n",
    "\n",
    "#@markdown ---------------------------\n",
    "\n",
    "\n",
    "#@markdown **************************************************** ACTIVATE TRAINING ***********************************************************\n",
    "\n",
    "def txtenc_train(Caption, stpsv, stp, MODELT_NAME, INSTANCE_DIR, CLASS_DIR, OUTPUT_DIR, PT, CT, Seed, Res_Int, precision, Learning_Rate_Def, Num_Class_Images_Def, Training_Steps):\n",
    "  print('\u001b[1;33mTraining the text encoder with regularization...\u001b[0m')\n",
    "  !accelerate launch --mixed_precision=$precision /workspace/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n",
    "    $Caption \\\n",
    "    --train_text_encoder \\\n",
    "    --dump_only_text_encoder \\\n",
    "    --pretrained_model_name_or_path=\"$MODEL_NAME\" \\\n",
    "    --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
    "    --class_data_dir=\"$CLASS_DIR\" \\\n",
    "    --output_dir=\"$OUTPUT_DIR\" \\\n",
    "    --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "    --instance_prompt=\"$PT\" \\\n",
    "    $ClassPromptVar \\\n",
    "    --seed=$Seed \\\n",
    "    --resolution=$Res_Int \\\n",
    "    --train_batch_size=1 \\\n",
    "    --gradient_accumulation_steps=1 --gradient_checkpointing \\\n",
    "    --use_8bit_adam \\\n",
    "    --learning_rate=$Learning_Rate_Def \\\n",
    "    --lr_scheduler=\"polynomial\" \\\n",
    "    --lr_warmup_steps=0 \\\n",
    "    --max_train_steps=$Training_Steps \\\n",
    "    --num_class_images=$Num_Class_Images_Def\n",
    "\n",
    "def unet_train(Caption, SESSION_DIR, stpsv, stp, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, CT, Seed, Res_Int, precision, GC, Learning_Rate_Def, Training_Steps):\n",
    "  #clear_output()\n",
    "  print('\u001b[1;33mTraining the unet...\u001b[0m')\n",
    "  !accelerate launch --mixed_precision=$precision /workspace/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n",
    "    $Caption \\\n",
    "    --train_only_unet \\\n",
    "    --Session_dir=$SESSION_DIR \\\n",
    "    --save_starting_step=$stpsv \\\n",
    "    --save_n_steps=$stp \\\n",
    "    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n",
    "    --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
    "    --output_dir=\"$OUTPUT_DIR\" \\\n",
    "    --instance_prompt=\"$PT\" \\\n",
    "    $ClassPromptVar \\\n",
    "    --seed=$Seed \\\n",
    "    --resolution=$Res_Int \\\n",
    "    --train_batch_size=1 \\\n",
    "    --gradient_accumulation_steps=1 $GC \\\n",
    "    --use_8bit_adam \\\n",
    "    --learning_rate=$Learning_Rate_Def \\\n",
    "    --lr_scheduler=\"polynomial\" \\\n",
    "    --lr_warmup_steps=10 \\\n",
    "    --max_train_steps=$Training_Steps\n",
    "\n",
    "if Enable_text_encoder_training:\n",
    "  txtenc_train(Caption, stpsv, stp, MODELT_NAME, INSTANCE_DIR, CLASS_DIR, OUTPUT_DIR, PT, CT, Seed, Res_Int, precision, Learning_Rate_Def, Num_Class_Images_Def, Training_Steps=stptxt)\n",
    "  unet_train(Caption, SESSION_DIR, stpsv, stp, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, CT, Seed, Res_Int, precision, GC, Learning_Rate_Def, Training_Steps)\n",
    "else:\n",
    "  !accelerate launch --mixed_precision=$precision /workspace/content/diffusers/examples/dreambooth/train_dreambooth.py \\\n",
    "    $Caption \\\n",
    "    $Textenc \\\n",
    "    --save_starting_step=$stpsv \\\n",
    "    --stop_text_encoder_training=$stptxt \\\n",
    "    --save_n_steps=$stp \\\n",
    "    --Session_dir=$SESSION_DIR \\\n",
    "    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n",
    "    --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
    "    --output_dir=\"$OUTPUT_DIR\" \\\n",
    "    --instance_prompt=\"$PT\" \\\n",
    "    $ClassPromptVar \\\n",
    "    --seed=$Seed \\\n",
    "    --resolution=$Res_Int \\\n",
    "    --train_batch_size=1 \\\n",
    "    --gradient_accumulation_steps=1 $GC \\\n",
    "    --use_8bit_adam \\\n",
    "    --learning_rate=$Learning_Rate_Def \\\n",
    "    --lr_scheduler=\"polynomial\" \\\n",
    "    --lr_warmup_steps=10 \\\n",
    "    --max_train_steps=$Training_Steps\n",
    "\n",
    "\n",
    "if os.path.exists('/workspace/content/models/'+INSTANCE_NAME+'/unet/diffusion_pytorch_model.bin'):\n",
    "  print(\"Almost done ...\")\n",
    "  %cd /workspace/content    \n",
    "  !wget -O convertosd.py https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/convertosd.py\n",
    "  #clear_output()\n",
    "  if precision==\"no\":\n",
    "    !sed -i '226s@.*@@' /workspace/content/convertosd.py\n",
    "  !sed -i '201s@.*@    model_path = \"{OUTPUT_DIR}\"@' /workspace/content/convertosd.py\n",
    "  !sed -i '202s@.*@    checkpoint_path= \"{SESSION_DIR}/{Session_Name}.ckpt\"@' /workspace/content/convertosd.py\n",
    "  !python /workspace/content/convertosd.py\n",
    "  #clear_output()\n",
    "  if os.path.exists(SESSION_DIR+\"/\"+INSTANCE_NAME+'.ckpt'):\n",
    "    if not os.path.exists(str(SESSION_DIR+'/tokenizer')):\n",
    "      !cp -R '/workspace/content/models/'$INSTANCE_NAME'/tokenizer' \"$SESSION_DIR\"\n",
    "    print(\"\u001b[1;32mDONE, the CKPT model is in the sessions folder\")\n",
    "  else:\n",
    "    print(\"\u001b[1;31mSomething went wrong\")\n",
    "    \n",
    "else:\n",
    "  print(\"\u001b[1;31mSomething went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download your new model\n",
    "\n",
    "%cd /kaggle/working\n",
    "clear.output()\n",
    "\n",
    "os.symlink(MDLPTH, 'db_checkpoint.ckpt')\n",
    "print(\"Remember! Your activation prompt is:\",PT,CT)\n",
    "FileLink(r'db_checkpoint.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
