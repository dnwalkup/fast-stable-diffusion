{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################################################ Dreambooth 4 Kaggle Mk2 #############################################################\n",
    "#\n",
    "# Based on u/Yacben's (https://github.com/TheLastBen/fast-stable-diffusion)\n",
    "# Adapated for Kaggle by u/shutonga (https://github.com/tuwonga/fast_Dreambooth_4_kaggle)\n",
    "# Mk2 by u/_rundown_\n",
    "#\n",
    "#-------------------------------------------------------------- GLOBAL VARIABLES ----------------------------------------------------------------#\n",
    "# You've got your images uploaded, you're settings are locked in, time to start this 'ish.\n",
    "#\n",
    "# Let's set some global variables that are important for the training of your model.\n",
    "#\n",
    "#\n",
    "# ** REMINDER: Best not to use spaces. **\n",
    "#\n",
    "\n",
    "Huggingface_Token = \"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\"\n",
    "# Downloading the Stable Diffusion model\n",
    "# ** NOTE: Leave EMPTY if you're using the v2 model **\n",
    "\n",
    "# To download the Stable Diffusion v1.5 checkpoint, a Hugging Face account is required and you must\n",
    "# accept the terms in https://huggingface.co/runwayml/stable-diffusion-v1-5\n",
    "\n",
    "# In your account settings > access tokens (https://huggingface.co/settings/tokens), you can create a new token. Then copy and paste it above\n",
    "# between the parenthesis.\n",
    "\n",
    "GPU_Select = \"RTX 3090\"\n",
    "# If you know your GPU, enter it above. Current support includes \"A100\", \"V100\", \"P100\", \"T4\".\n",
    "# \"RTX 3090\" and \"A5000\" are available for CUDA11.7+Python3.10 on Linux.\n",
    "\n",
    "Model_Version = \"1.5\"\n",
    "# \"1.5\", \"V2-512px\", \"V2-768px\"\n",
    "\n",
    "Session_Name = \"zza_test\"\n",
    "# Name your session. If it already exists, it will load it to continue.\n",
    "\n",
    "Training_Type = \"artwork style\"\n",
    "# Current options are: person or artwork style. Insert word in the quotes.\n",
    "\n",
    "Class_Token = True\n",
    "# Class token is a part of your activation prompt. True or False. If true, your class token will be the same as your Training Type.\n",
    "\n",
    "PT = \"zanzedegiazadi\"\n",
    "# Prompt token. This is what you'll add to your SD prompt to activate your style/person  \n",
    "\n",
    "Crop_images = False\n",
    "# Do you want the app to manually crop your images? True or False. Default is False (recommended you follow the above directions).\n",
    "\n",
    "Crop_size = \"512\"\n",
    "# Crop size is your ending image size. Advanced users can modify up to 1024. Default value is 512.\n",
    "\n",
    "Training_Steps = 16000\n",
    "# Number of instance images * 100 (e.g. if you use 30 images, use 3000 steps). Default value 1000.\n",
    "\n",
    "Learning_Rate_Def = 1e-6\n",
    "# Some have good results with 2e-6 and other options. Default value 1e-6.\n",
    "\n",
    "Save_Checkpoint_Every = 2000\n",
    "# Minimum 200 steps between each save.\n",
    "# ** NOTE: Remember you only have 20GB space on your Kaggle drive and each CKPT is > 2GB. **\n",
    "\n",
    "GDrive_Inst_Images = \"https://drive.google.com/file/d/1cK0gV8WtByP2Hr39F0mRf2176sb8pHi5/view?usp=sharing\"\n",
    "# Link to your images on your GDrive\n",
    "\n",
    "\n",
    "#-------------------------------------------------------- GLOBAL VARIABLES - ERROR OPTIONS ---------------------------------------------------------#\n",
    " \n",
    "Reduce_memory_usage = True\n",
    "# Enables gradient checkpointing. If you're getting memory issues, change this to True (slower speed but memory effecient).\n",
    "\n",
    "Compatibility_Mode = False\n",
    "# Enable only if you're getting model conversion errors for advanced custom CKPTs. True or False.\n",
    "\n",
    "\n",
    "#----------------------------------------------------- GLOBAL VARIABLES - ADVANCED OPTIONS --------------------------------------------------------#\n",
    "#\n",
    "# ** NOTE: If you're unsure about these settings, leave them as-is.\n",
    "\n",
    "fp16 = True\n",
    "# Enable/disable half-precision, disabling it will double the training time and produce 4.7Gb checkpoints.\n",
    "\n",
    "Seed = '1245682727'\n",
    "# Leave empty for a random seed.\n",
    "\n",
    "Session_Link_optional = \"\"\n",
    "# Import a session from another gdrive\n",
    "# The shared gdrive link must point to the specific session's folder that contains the trained CKPT, remove the intermediary CKPT if any exist.\n",
    "\n",
    "Resume_Training = False\n",
    "# If starting from scratch, this should be False. If you're resuming a training, True.\n",
    "\n",
    "\n",
    "# Options for using a different model:\n",
    "\n",
    "Path_to_HuggingFace= \"\" #Use the format \"profile/model\" (e.g. runwayml/stable-diffusion-v1-5)\n",
    "\n",
    "CKPT_Path = \"\" #Kaggle path to an uploaded CKPT\n",
    "\n",
    "CKPT_Link = \"\" #A CKPT direct link, huggingface CKPT link, or a shared CKPT from gdrive.\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "#-------------------------------------------------------------------- IMPORTS ---------------------------------------------------------------------#\n",
    "# Now for the code. Don't change anything here:\n",
    "#\n",
    "\n",
    "import os, sys, gc, time, shutil, random, wget, ftfy\n",
    "\n",
    "from subprocess import getoutput\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "from IPython.utils import capture\n",
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "from IPython.display import Javascript\n",
    "from IPython.display import FileLink\n",
    "\n",
    "!mkdir -p /workspace/content/Sessions\n",
    "!mkdir /workspace/content/models\n",
    "!mkdir /workspace/content/diffusers\n",
    "\n",
    "# Define working directories\n",
    "Session_Name = Session_Name.replace(\" \",\"_\")\n",
    "\n",
    "WORKSPACE = '/workspace/content'\n",
    "\n",
    "OUTPUT_DIR = '/workspace/content/models/' + Session_Name\n",
    "SESSION_DIR = '/workspace/content/Sessions/' + Session_Name\n",
    "INSTANCE_DIR = SESSION_DIR + '/instance_images'\n",
    "CLASS_DIR = SESSION_DIR + '/regularization_images'\n",
    "MDLPTH = str(SESSION_DIR + '/' + Session_Name + '.ckpt')\n",
    "\n",
    "# Double check GPU card\n",
    "GPU_CardName = getoutput('nvidia-smi --query-gpu=name --format=csv,noheader')\n",
    "\n",
    "if GPU_Select in GPU_CardName:\n",
    "    clear_output()\n",
    "    print('\u001b[1;32mInitial setup complete, variables set, please move on to the next cell !')\n",
    "else:\n",
    "    clear_output()\n",
    "    print('\u001b[1;32mApparent GPU mismatch. Please double check that you input the correct GPU into the \"GPU_Select\" variable above. Your node is reporting this GPU: ', GPU_CardName)\n",
    "    print('\u001b[1;32mInitial setup complete, variables set, you may move on to the next cell if you are sure of your GPU selection.')\n",
    "\n",
    "#\n",
    "#\n",
    "#\n",
    "# ** NOTE: YOU MUST RUN THIS CELL FOR THE PROGRAM TO EXECUTE AND REMEMBER YOUR SETTINGS! **\n",
    "#\n",
    "#\n",
    "####################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Diffusers and dependencies\n",
    "\n",
    "%cd /workspace/content\n",
    "\n",
    "!apt update\n",
    "!apt-get -y install git\n",
    "!apt-get -y install git-lfs\n",
    "!git lfs install\n",
    "\n",
    "!pip install diffusers\n",
    "!pip install accelerate\n",
    "!pip install modelcards\n",
    "!pip install bitsandbytes\n",
    "!pip install --upgrade requests\n",
    "\n",
    "%cd /workspace/content/diffusers\n",
    "!wget https://github.com/huggingface/diffusers/raw/6c56f05097f7d3c561f02dc1c27e3dd7e9f88ce1/examples/dreambooth/train_dreambooth.py\n",
    "!wget -O convertosd.py https://github.com/huggingface/diffusers/raw/main/scripts/convert_diffusers_to_original_stable_diffusion.py\n",
    "\n",
    "#for new xformers\n",
    "#torch==1.13.0\n",
    "#nvidia-cuda-nvrtc-cu11==11.7.99\n",
    "#nvidia-cublas-cu11==11.10.3.66\n",
    "#nvidia-cudnn-cu11==8.5.0.96\n",
    "#nvidia-cuda-runtime-cu11==11.7.99\n",
    "\n",
    "clear_output()\n",
    "print('\u001b[1;32mDependencies installed, please move on to the next cell !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install xformers\n",
    "\n",
    "if (GPU_Select == \"T4\"):\n",
    "  %pip install https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/T4/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "  !pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "  clear_output()\n",
    "  print('\u001b[1;32mInstalled xformers for your selected GPU to speed up training your model, please move on to the next cell !')\n",
    "\n",
    "elif (GPU_Select == \"P100\"):\n",
    "  %pip install https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/P100/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "  !pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "  clear_output()\n",
    "  print('\u001b[1;32mInstalled xformers for your selected GPU to speed up training your model, please move on to the next cell !')\n",
    "\n",
    "elif (GPU_Select == \"V100\"):\n",
    "  %pip install https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/V100/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "  !pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "  clear_output()\n",
    "  print('\u001b[1;32mInstalled xformers for your selected GPU to speed up training your model, please move on to the next cell !')\n",
    "\n",
    "elif (GPU_Select == \"A100\"):\n",
    "  %pip install https://github.com/TheLastBen/fast-stable-diffusion/raw/main/precompiled/A100/xformers-0.0.13.dev0-py3-none-any.whl\n",
    "  !pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "  clear_output()\n",
    "  print('\u001b[1;32mInstalled xformers for your selected GPU to speed up training your model, please move on to the next cell !')\n",
    "\n",
    "elif (GPU_Select == \"A5000\"):\n",
    "  %pip install https://huggingface.co/dnwalkup/xformers-precompiles/resolve/main/A5000/xformers-0.0.15.dev0%2B4c06c79.d20221205-cp38-cp38-linux_x86_64.whl\n",
    "  !pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "  #clear_output()\n",
    "  print('\u001b[1;32mInstalled xformers for your A5000 GPU to speed up training your model, please move on to the next cell !')\n",
    "\n",
    "elif (GPU_Select == \"RTX 3090\"):\n",
    "  %pip install https://huggingface.co/dnwalkup/xformers-precompiles/resolve/main/RTX3090/xformers-0.0.15.dev0%2Bc733c99.d20221129-cp38-cp38-linux_x86_64.whl\n",
    "  !pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "  #clear_output()\n",
    "  print('\u001b[1;32mInstalled xformers for your selected GPU to speed up training your model, please move on to the next cell !')\n",
    "\n",
    "else:\n",
    "    print('\u001b[1;31mit seems that your GPU is not supported at the moment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell downloads the Stable Diffusion checkpoint to your working folder.\n",
    "#\n",
    "# ** NOTE: Skip this cell if you are loading a previous session (you don't need to download it more than once)\n",
    "#\n",
    "\n",
    "with capture.capture_output() as cap: \n",
    "  %cd /workspace/content/\n",
    "\n",
    "if Model_Version == \"V2-512px\" or Model_Version == \"V2-768px\":\n",
    "  Huggingface_Token = \"\"\n",
    "\n",
    "token = Huggingface_Token\n",
    "\n",
    "\n",
    "def downloadmodel():\n",
    "  \n",
    "  token = Huggingface_Token\n",
    "  \n",
    "  if os.path.exists('/workspace/content/stable-diffusion-v1-5'):\n",
    "    !rm -r /workspace/content/stable-diffusion-v1-5\n",
    "\n",
    "  %cd /workspace/content/\n",
    "  !mkdir /workspace/content/stable-diffusion-v1-5\n",
    "  %cd /workspace/content/stable-diffusion-v1-5\n",
    "\n",
    "  !git init\n",
    "  !git lfs install --system --skip-repo\n",
    "  !git remote add -f origin  \"https://USER:{token}@huggingface.co/runwayml/stable-diffusion-v1-5\"\n",
    "  !git config core.sparsecheckout true\n",
    "  #!echo \"v1-5-pruned-emaonly.ckpt\" >> .git/info/sparse-checkout\n",
    "  #!echo \"v1-inference.yaml\" >> .git/info/sparse-checkout\n",
    "  !echo \"feature_extractor\" >> .git/info/sparse-checkout\n",
    "  !echo \"safety_checker\" >> .git/info/sparse-checkout\n",
    "  !echo \"scheduler\" >> .git/info/sparse-checkout\n",
    "  !echo \"text_encoder\" >> .git/info/sparse-checkout\n",
    "  !echo \"tokenizer\" >> .git/info/sparse-checkout\n",
    "  !echo \"unet\" >> .git/info/sparse-checkout\n",
    "  !echo \"vae\" >> .git/info/sparse-checkout\n",
    "  !echo \".gitatributes\" >> .git/info/sparse-checkout\n",
    "  !echo \"model_index.json\" >> .git/info/sparse-checkout\n",
    "  !echo \"v1-inference.yaml\" >> .git/info/sparse-checkout\n",
    "  !git pull origin main -v\n",
    "\n",
    "  #!gdown --fuzzy https://drive.google.com/file/d/1LAJTVMBuWHwIC9ixB9ZBW9U-elNsmlLk/view?usp=sharing -O v1-5-pruned-emaonly.ckpt\n",
    "  #!wget -O v1-inference.yaml https://github.com/dnwalkup/fast-stable-diffusion/raw/main/Dependencies/v1-inference.yaml\n",
    "\n",
    "  #PATHTEST = \"/workspace/content/stable-diffusion-v1-5\"\n",
    "  #CKPT_PATHTEST = \"/workspace/content/stable-diffusion-v1-5/v1-5-pruned-emaonly.ckpt\"\n",
    "  #YAML_PATHTEST = \"/workspace/content/stable-diffusion-v1-5/v1-inference.yaml\"\n",
    "\n",
    "  #!pip install OmegaConf #For converting ckpt to diffusers (in docker)\n",
    "  #!pip install pytorch-lightning #For converting ckpt to diffusers\n",
    "\n",
    "  #%cd /workspace/content/stable-diffusion-v1-5\n",
    "\n",
    "  #!wget -O convertsdd.py https://github.com/dnwalkup/fast-stable-diffusion/raw/main/Dependencies/convertsdd.py\n",
    "  #!python3 convertsdd.py --checkpoint_path=$CKPT_PATHTEST --original_config_file=$YAML_PATHTEST --scheduler_type='ddim' --image_size=512 --prediction_type='epsilon' --dump_path=$PATHTEST\n",
    "  \n",
    "  #clear_output()\n",
    "  print('\u001b[1;32mDownloaded SDv1.5 from RunwayML on Hugging Face, please move on to the next cell !')\n",
    "  \n",
    "  if not os.path.exists('/workspace/content/stable-diffusion-v1-5/unet/diffusion_pytorch_model.bin'):\n",
    "    print('\u001b[1;31mMake sure you accepted the terms in https://huggingface.co/runwayml/stable-diffusion-v1-5')\n",
    "    time.sleep(2)\n",
    "\n",
    "        \n",
    "def newdownloadmodel():\n",
    "\n",
    "  %cd /workspace/content/\n",
    "\n",
    "  # Function to download SDv2 from Hugging Face\n",
    "\n",
    "  !mkdir /workspace/content/stable-diffusion-v2-768\n",
    "  %cd /workspace/content/stable-diffusion-v2-768\n",
    "  \n",
    "  !git init\n",
    "  !git lfs install --system --skip-repo\n",
    "  !git remote add -f origin  \"https://USER:{token}@huggingface.co/stabilityai/stable-diffusion-2\"\n",
    "  !git config core.sparsecheckout true\n",
    "  !echo \"768-v-ema.ckpt\" >> .git/info/sparse-checkout\n",
    "  !echo \"LICENSE-MODEL\" >> .git/info/sparse-checkout\n",
    "  !echo \"README.md\" >> .git/info/sparse-checkout\n",
    "  !echo \"model-variants.jpg\" >> .git/info/sparse-checkout\n",
    "  !git pull origin main -v\n",
    "\n",
    "  # GDrive function for faster speeds\n",
    "  #!gdown --fuzzy https://drive.google.com/file/d/1NnpSM971X5vAbGgvFMaMbKLvqAfEUSt8/view?usp=sharing -O sdv2.zip\n",
    "  #!unzip sdv2.zip\n",
    "  #!rm sdv2.zip\n",
    "  #!mv /workspace/content/stable-diffusion-2 /workspace/content/stable-diffusion-v2-768\n",
    "  \n",
    "  clear_output()\n",
    "  print('\u001b[1;32mDownloaded SD V2 768 model from StabilityAI on Hugging Face, please move on to the next cell !')\n",
    "\n",
    "\n",
    "def newdownloadmodelb():\n",
    "\n",
    "  %cd /workspace/content/\n",
    "  clear_output()\n",
    "\n",
    "  !mkdir /workspace/content/stable-diffusion-v2-512\n",
    "  %cd /workspace/content/stable-diffusion-v2-512\n",
    "\n",
    "  !git init\n",
    "  !git lfs install --system --skip-repo\n",
    "  !git remote add -f origin  \"https://USER:{token}@huggingface.co/stabilityai/stable-diffusion-2-base\"\n",
    "  !git config core.sparsecheckout true\n",
    "  !echo \"512-base-ema.ckpt\" >> .git/info/sparse-checkout\n",
    "  !echo \"README.md\" >> .git/info/sparse-checkout\n",
    "  !git pull origin main -v\n",
    "\n",
    "  clear_output()\n",
    "  print('\u001b[1;32mDownloaded SD V2 512 model from StabilityAI on Hugging Face, please move on to the next cell !')\n",
    "\n",
    "        \n",
    "if Path_to_HuggingFace != \"\":\n",
    "  \n",
    "  if os.path.exists('/workspace/content/stable-diffusion-custom'):\n",
    "    !rm -r /contkaggle/working/contentent/stable-diffusion-custom\n",
    "  clear_output()\n",
    "\n",
    "  %cd /workspace/content/\n",
    "  clear_output()\n",
    "\n",
    "  !mkdir /workspace/content/stable-diffusion-custom\n",
    "  %cd /workspace/content/stable-diffusion-custom\n",
    "\n",
    "  !git init\n",
    "  !git lfs install --system --skip-repo\n",
    "  !git remote add -f origin  \"https://USER:{token}@huggingface.co/{Path_to_HuggingFace}\"\n",
    "  !git config core.sparsecheckout true\n",
    "  !echo -e \"scheduler\\ntext_encoder\\ntokenizer\\nunet\\nmodel_index.json\" > .git/info/sparse-checkout\n",
    "  !git pull origin main\n",
    "\n",
    "  if os.path.exists('/workspace/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n",
    "    !git clone \"https://USER:{token}@huggingface.co/stabilityai/sd-vae-ft-mse\"\n",
    "    !mv /workspace/content/stable-diffusion-custom/sd-vae-ft-mse /workspace/content/stable-diffusion-custom/vae\n",
    "    !rm -r /workspace/content/stable-diffusion-custom/.git\n",
    "    %cd /workspace/content/stable-diffusion-custom\n",
    "    !rm model_index.json\n",
    "    time.sleep(1)\n",
    "    wget.download('https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/model_index.json')\n",
    "    !sed -i 's@\"clip_sample\": false@@g' /workspace/content/stable-diffusion-custom/scheduler/scheduler_config.json\n",
    "    !sed -i 's@\"trained_betas\": null,@\"trained_betas\": null@g' /workspace/content/stable-diffusion-custom/scheduler/scheduler_config.json\n",
    "    !sed -i 's@\"sample_size\": 256,@\"sample_size\": 512,@g' /workspace/content/stable-diffusion-custom/vae/config.json    \n",
    "    %cd /workspace/content/ \n",
    "    MODEL_NAME = \"/workspace/content/stable-diffusion-custom\"\n",
    "\n",
    "    clear_output()\n",
    "    print('\u001b[1;32mDownloaded your custom model from Hugging Face, please move on to the next cell !')\n",
    "\n",
    "  else:\n",
    "    while not os.path.exists('/workspace/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n",
    "          print('\u001b[1;32mCheck the link you provided')\n",
    "          time.sleep(5)\n",
    "\n",
    "\n",
    "elif CKPT_Path != \"\":\n",
    "\n",
    "  if os.path.exists('/workspace/content/stable-custom'):\n",
    "    !rm -r /workspace/content/stable-diffusion-custom\n",
    "  \n",
    "  if os.path.exists(str(CKPT_Path)):\n",
    "    !mkdir /workspace/content/stable-diffusion-custom\n",
    "    \n",
    "    with capture.capture_output() as cap:\n",
    "      if Compatibility_Mode:\n",
    "        !wget https://raw.githubusercontent.com/huggingface/diffusers/039958eae55ff0700cfb42a7e72739575ab341f1/scripts/convert_original_stable_diffusion_to_diffusers.py\n",
    "        !python /workspace/content/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path \"$CKPT_Path\" --dump_path /workspace/content/stable-diffusion-custom\n",
    "        !rm /workspace/content/convert_original_stable_diffusion_to_diffusers.py\n",
    "      else:           \n",
    "        !python /workspace/content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path \"$CKPT_Path\" --dump_path /workspace/content/stable-diffusion-custom\n",
    "    \n",
    "    if os.path.exists('/conkaggle/working/contenttent/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n",
    "      !rm /workspace/content/v1-inference.yaml\n",
    "      clear_output()\n",
    "      MODEL_NAME = \"/workspace/content/stable-diffusion-custom\"\n",
    "      print('\u001b[1;32mCheckpoint converted from custom path, please move on to the next cell !')\n",
    "    else:\n",
    "      !rm /workspace/content/convert_original_stable_diffusion_to_diffusers.py\n",
    "      !rm /workspace/content/v1-inference.yaml\n",
    "      !rm -r /workspace/content/stable-diffusion-custom\n",
    "      while not os.path.exists('/workspace/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n",
    "        print('\u001b[1;32mConversion error, Insufficient RAM or corrupt CKPT, use a 4GB CKPT instead of 7GB')\n",
    "        time.sleep(5)\n",
    "  \n",
    "  else:\n",
    "    while not os.path.exists(str(CKPT_Path)):\n",
    "       print('\u001b[1;32mWrong path, use the file explorer to copy the path')\n",
    "       time.sleep(5)\n",
    "\n",
    "\n",
    "elif CKPT_Link != \"\":   \n",
    "    \n",
    "    if os.path.exists('/workspace/content/stable-diffusion-custom'):\n",
    "      !rm -r /workspace/content/stable-diffusion-custom   \n",
    "    \n",
    "    !gdown --fuzzy $CKPT_Link -O model.ckpt    \n",
    "    \n",
    "    if os.path.exists('/workspace/content/model.ckpt'):\n",
    "      if os.path.getsize(\"/workspace/content/model.ckpt\") > 1810671599:\n",
    "        !mkdir /workspace/content/stable-diffusion-custom\n",
    "        \n",
    "        with capture.capture_output() as cap: \n",
    "          if Compatibility_Mode:\n",
    "            !wget https://raw.githubusercontent.com/huggingface/diffusers/039958eae55ff0700cfb42a7e72739575ab341f1/scripts/convert_original_stable_diffusion_to_diffusers.py\n",
    "            !python /workspace/content/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path /workspace/content/model.ckpt --dump_path /workspace/content/stable-diffusion-custom\n",
    "            !rm /workspace/content/convert_original_stable_diffusion_to_diffusers.py            \n",
    "          else:           \n",
    "            !python /workspace/content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path /workspace/content/model.ckpt --dump_path /workspace/content/stable-diffusion-custom\n",
    "        \n",
    "        if os.path.exists('/workspace/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n",
    "          clear_output()\n",
    "          MODEL_NAME = \"/workspace/content/stable-diffusion-custom\"\n",
    "          print('\u001b[1;32mCheckpoint converted from custom path, please move on to the next cell !')\n",
    "          !rm /workspace/content/v1-inference.yaml\n",
    "          !rm /workspace/content/model.ckpt\n",
    "        \n",
    "        else:\n",
    "          if os.path.exists('/workspace/content/v1-inference.yaml'):\n",
    "            !rm /workspace/content/v1-inference.yaml\n",
    "          !rm /workspace/content/convert_original_stable_diffusion_to_diffusers.py\n",
    "          !rm -r /workspace/content/stable-diffusion-custom\n",
    "          !rm /workspace/content/model.ckpt\n",
    "          while not os.path.exists('/workspace/content/stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n",
    "            print('\u001b[1;32mConversion error, Insufficient RAM or corrupt CKPT, use a 4GB CKPT instead of 7GB')\n",
    "            time.sleep(5)\n",
    "      \n",
    "      else:\n",
    "        while os.path.getsize('/workspace/content/model.ckpt') < 1810671599:\n",
    "           print('\u001b[1;32mWrong link, check that the link is valid')\n",
    "           time.sleep(5)\n",
    "\n",
    "\n",
    "else:\n",
    "  \n",
    "  if Model_Version == \"V2-512px\":\n",
    "    if not os.path.exists('/workspace/content/stable-diffusion-v2-512'):\n",
    "      newdownloadmodelb()\n",
    "      MODEL_NAME=\"/workspace/content/stable-diffusion-v2-512\"\n",
    "    else:\n",
    "      print(\"\u001b[1;32mThe v2-512px model already exist, using this model.\")      \n",
    "  \n",
    "  elif Model_Version == \"V2-768px\":\n",
    "    if not os.path.exists('/workspace/content/stable-diffusion-v2-768'):   \n",
    "      newdownloadmodel()\n",
    "      MODEL_NAME = \"/workspace/content/stable-diffusion-v2-768\"\n",
    "    else:\n",
    "      print(\"\u001b[1;32mThe v2-768px model already exist, using this model.\")\n",
    "\n",
    "  else:\n",
    "    if not os.path.exists('/workspace/content/stable-diffusion-v1-5'):\n",
    "      downloadmodel()\n",
    "      MODEL_NAME = \"/workspace/content/stable-diffusion-v1-5\"\n",
    "    else:\n",
    "      print(\"\u001b[1;32mThe v1.5 model already exist, using this model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create or load a session\n",
    "\n",
    "try:\n",
    "  MODEL_NAME\n",
    "except:\n",
    "  MODEL_NAME = \"\"\n",
    "  print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n",
    "\n",
    "\n",
    "if Session_Link_optional != \"\":\n",
    "  print('\u001b[1;32mDownloading session...')\n",
    "\n",
    "with capture.capture_output() as cap:\n",
    "  %cd /workspace/content\n",
    "  \n",
    "  if Session_Link_optional != \"\":\n",
    "    if not os.path.exists(str(WORKSPACE+'/Sessions')):\n",
    "      %mkdir -p $WORKSPACE'/Sessions'\n",
    "      time.sleep(1)\n",
    "    %cd $WORKSPACE'/Sessions'\n",
    "    !gdown --folder --remaining-ok -O $Session_Name $Session_Link_optional\n",
    "    %cd $Session_Name\n",
    "    !rm -r instance_images\n",
    "    !rm -r regularization_images\n",
    "    !unzip instance_images.zip\n",
    "    !mv *.ckpt $Session_Name\".ckpt\"\n",
    "    %cd /workspace/content\n",
    "\n",
    "\n",
    "if os.path.exists(str(SESSION_DIR)):\n",
    "  if not os.path.exists(MDLPTH) and '.ckpt' in str([ckpt for ckpt in listdir(SESSION_DIR) if ckpt.split(\".\")[-1]==\"ckpt\"]):  \n",
    "    print('\u001b[1;32mSkipping the intermediary checkpoints.')\n",
    "\n",
    "    \n",
    "if os.path.exists(str(SESSION_DIR)) and not os.path.exists(MDLPTH):\n",
    "  print('\u001b[1;32mLoading session with no previous model, using the original model or the custom downloaded model')\n",
    "  if MODEL_NAME == \"\":\n",
    "    print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n",
    "  else:\n",
    "    print('\u001b[1;32mSession Loaded, proceed to uploading instance images')\n",
    "\n",
    "\n",
    "elif os.path.exists(MDLPTH):\n",
    "  print('\u001b[1;32mSession found, loading the trained model ...')\n",
    "  %mkdir -p \"$OUTPUT_DIR\"\n",
    "  !python /workspace/content/diffusers/scripts/convert_original_stable_diffusion_to_diffusers.py --checkpoint_path \"$MDLPTH\" --dump_path \"$OUTPUT_DIR\" --session_dir \"$SESSION_DIR\"\n",
    "  if os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n",
    "    resume = True    \n",
    "    !rm /workspace/content/v1-inference.yaml\n",
    "    clear_output()\n",
    "    print('\u001b[1;32mSession loaded.')\n",
    "  else:     \n",
    "    !rm /workspace/content/v1-inference.yaml\n",
    "    if not os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n",
    "      print('\u001b[1;31mConversion error, if the error persists, remove the CKPT file from the current session folder')\n",
    "\n",
    "\n",
    "elif not os.path.exists(str(SESSION_DIR)):\n",
    "    %mkdir -p \"$INSTANCE_DIR\"\n",
    "    print('\u001b[1;32mCreating session...')\n",
    "    if MODEL_NAME == \"\":\n",
    "      print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n",
    "    else:\n",
    "      print('\u001b[1;32mSession created, proceed to uploading instance images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download your instance images from Google Drive\n",
    "\n",
    "!mkdir /workspace/my\n",
    "%cd /workspace/my\n",
    "\n",
    "!gdown --fuzzy $GDrive_Inst_Images -O user_images.zip\n",
    "!unzip user_images.zip\n",
    "!rm user_images.zip\n",
    "\n",
    "for instance_items in os.scandir(os.getcwd()):\n",
    "    if instance_items.is_dir():\n",
    "        INSTANCE_DIR_TMP = instance_items.path\n",
    "\n",
    "os.rename(INSTANCE_DIR_TMP,\"user_images\")\n",
    "\n",
    "%cd /workspace/my/user_images\n",
    "!find . -name \"* *\" -type f | rename 's/ /_/g'\n",
    "\n",
    "del INSTANCE_DIR_TMP\n",
    "gc.collect()\n",
    "\n",
    "clear_output()\n",
    "print('\u001b[1;32mInstance images copied to working directory, please move on to the next cell !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download and initialize regularization images \n",
    "\n",
    "if Model_Version == \"V2-768px\":\n",
    "    if Training_Type.lower() == \"person\":\n",
    "        Num_Class_Images_Def = 1000\n",
    "        %cd $SESSION_DIR\n",
    "        !rm -r $SESSION_DIR/*\n",
    "        !gdown --fuzzy https://drive.google.com/file/d/1--UP6J_JlixzAKYF8x327RidnsRphAvK/view?usp=share_link -O Regz.zip\n",
    "        !unzip Regz.zip\n",
    "        !rm Regz.zip\n",
    "    else:\n",
    "        Num_Class_Images_Def = 1000\n",
    "        %cd $SESSION_DIR\n",
    "        !rm -r $SESSION_DIR/*\n",
    "        #!gdown --fuzzy https://drive.google.com/file/d/1zZXLMQ-Uzpsv-NCsvvETtaeZhaYnTxBW/view?usp=sharing -O Regz.zip\n",
    "        !wget -O Regz.zip https://dl.dropboxusercontent.com/s/ng4dropggvem3nd/artwork_style768.zip?dl=0\n",
    "        !unzip Regz.zip\n",
    "        !rm Regz.zip\n",
    "\n",
    "else:\n",
    "    if Training_Type.lower() == \"person\":\n",
    "        Num_Class_Images_Def = 1000\n",
    "        %cd $SESSION_DIR\n",
    "        !rm -r $SESSION_DIR/*\n",
    "        !gdown --fuzzy https://drive.google.com/file/d/1dsfY9SF7992t7cc8O-DY1UYCW6Jn6hKz/view?usp=share_link -O Regz.zip\n",
    "        !unzip Regz.zip\n",
    "        !rm Regz.zip\n",
    "    else:\n",
    "        Num_Class_Images_Def = 1001\n",
    "        %cd $SESSION_DIR\n",
    "        !rm -r $SESSION_DIR/*\n",
    "        !gdown --fuzzy https://drive.google.com/file/d/1d0KsluHx-ZaYCGThxZMuTxVbor2pROlF/view?usp=share_link -O Regz.zip\n",
    "        !unzip Regz.zip\n",
    "        !rm Regz.zip\n",
    "\n",
    "for reg_items in os.scandir(os.getcwd()):\n",
    "    if reg_items.is_dir():\n",
    "        REG_DIR = reg_items.path\n",
    "\n",
    "os.rename(REG_DIR,\"reg_images\")\n",
    "\n",
    "CLASS_DIR = SESSION_DIR + '/reg_images'\n",
    "\n",
    "os.chdir(CLASS_DIR)\n",
    "!find . -name \"* *\" -type f | rename 's/ /_/g'\n",
    "\n",
    "del REG_DIR\n",
    "gc.collect()\n",
    "\n",
    "clear_output()\n",
    "print('\u001b[1;32mRegularization images initialized, please move on to the next cell !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance images modifications including crop.\n",
    "#\n",
    "# ** NOTE: EVEN IF NOT CROPPING, THIS CELL MUST BE RUN **\n",
    "\n",
    "%cd /workspace/content\n",
    "\n",
    "Remove_existing_instance_images = True #@param{type: 'boolean'}\n",
    "#@markdown - Uncheck the box to keep the existing instance images.\n",
    "\n",
    "if Remove_existing_instance_images:\n",
    "  if os.path.exists(str(INSTANCE_DIR)):\n",
    "    !rm -r \"$INSTANCE_DIR\"\n",
    "\n",
    "if not os.path.exists(str(INSTANCE_DIR)):\n",
    "  %mkdir -p \"$INSTANCE_DIR\"\n",
    "\n",
    "IMAGES_FOLDER_OPTIONAL = \"/workspace/my\" #@param{type: 'string'} this is the working directory for your Kaggle instance images. DO NOT CHANGE.\n",
    "\n",
    "#@markdown - Crop script\n",
    "\n",
    "Crop_size = int(Crop_size)\n",
    "\n",
    "if IMAGES_FOLDER_OPTIONAL != \"\":\n",
    "  if Crop_images:\n",
    "    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n",
    "      extension = filename.split(\".\")[1]\n",
    "      identifier = filename.split(\".\")[0]\n",
    "      new_path_with_file = os.path.join(INSTANCE_DIR, filename)\n",
    "      file = Image.open(IMAGES_FOLDER_OPTIONAL+\"/\"+filename)\n",
    "      width, height = file.size\n",
    "      if file.size != (Crop_size, Crop_size):      \n",
    "        side_length = min(width, height)\n",
    "        left = (width - side_length)/2\n",
    "        top = (height - side_length)/2\n",
    "        right = (width + side_length)/2\n",
    "        bottom = (height + side_length)/2\n",
    "        image = file.crop((left, top, right, bottom))\n",
    "        image = image.resize((Crop_size, Crop_size))\n",
    "        image.save(new_path_with_file, format=\"PNG\", quality = 100)\n",
    "      else:\n",
    "        !cp \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n",
    "\n",
    "  else:\n",
    "    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n",
    "      %cp -r \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n",
    " \n",
    "  print('\\n\u001b[1;32mComplete!')\n",
    "\n",
    "\n",
    "elif IMAGES_FOLDER_OPTIONAL == \"\":\n",
    "  uploaded = files.upload()\n",
    "  if Crop_images:\n",
    "    for filename in tqdm(uploaded.keys(), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n",
    "      shutil.move(filename, INSTANCE_DIR)\n",
    "      extension = filename.split(\".\")[1]\n",
    "      identifier = filename.split(\".\")[0]\n",
    "      new_path_with_file = os.path.join(INSTANCE_DIR, filename)\n",
    "      file = Image.open(new_path_with_file)\n",
    "      width, height = file.size\n",
    "      if file.size != (Crop_size, Crop_size):        \n",
    "        side_length = min(width, height)\n",
    "        left = (width - side_length)/2\n",
    "        top = (height - side_length)/2\n",
    "        right = (width + side_length)/2\n",
    "        bottom = (height + side_length)/2\n",
    "        image = file.crop((left, top, right, bottom))\n",
    "        image = image.resize((Crop_size, Crop_size))\n",
    "        image.save(new_path_with_file, format=\"PNG\", quality = 100)\n",
    "\n",
    "      else:\n",
    "          image.save(new_path_with_file, format=extension.upper())\n",
    "      clear_output()\n",
    "  else:\n",
    "    for filename in tqdm(uploaded.keys(), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n",
    "      shutil.move(filename, INSTANCE_DIR)\n",
    "      clear_output()\n",
    "\n",
    "  print('\\n\u001b[1;32mDone!')\n",
    "\n",
    "with capture.capture_output() as cap:\n",
    "  %cd \"$INSTANCE_DIR\"\n",
    "  !find . -name \"* *\" -type f | rename 's/ /-/g'\n",
    "  %cd /content\n",
    "  if os.path.exists(INSTANCE_DIR+\"/.ipynb_checkpoints\"):\n",
    "    %rm -r INSTANCE_DIR+\"/.ipynb_checkpoints\"    \n",
    "\n",
    "  %cd $SESSION_DIR\n",
    "  !rm instance_images.zip\n",
    "  !zip -r instance_images instance_images\n",
    "\n",
    "%cd $INSTANCE_DIR\n",
    "\n",
    "for all_dirs in os.scandir(os.getcwd()):\n",
    "    if all_dirs.is_dir():\n",
    "        INSTANCE_DIR = all_dirs.path\n",
    "        \n",
    "del all_dirs\n",
    "gc.collect()\n",
    "\n",
    "clear_output()\n",
    "print('\u001b[1;32mDone, time to start training ! Move on to the next cell !')\n",
    "\n",
    "print(INSTANCE_DIR)\n",
    "\n",
    "%cd /workspace/content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START DREAMBOOTH TRAINING !!\n",
    "\n",
    "while not Resume_Training and MODEL_NAME == \"\":\n",
    "  print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n",
    "  time.sleep(5)\n",
    "\n",
    "\n",
    "MODELT_NAME = MODEL_NAME\n",
    "\n",
    "Resolution = Crop_size\n",
    "\n",
    "Res_Int = int(Resolution)\n",
    "\n",
    "GC = \"\"\n",
    "if Reduce_memory_usage or Resolution != \"512\":\n",
    "  GC = \"--gradient_checkpointing\"\n",
    "\n",
    "if Seed == '' or Seed == '0':\n",
    "  Seed = random.randint(1, 999999)\n",
    "else:\n",
    "  Seed = int(Seed)\n",
    "\n",
    "if fp16:\n",
    "  prec = \"fp16\"\n",
    "else:\n",
    "  prec = \"no\"\n",
    "\n",
    "precision = prec\n",
    "\n",
    "\n",
    "CT = \"\"\n",
    "ClassPromptVar = ''\n",
    "\n",
    "if Class_Token:\n",
    "  CT = Training_Type\n",
    "  ClassPromptVar = '--class_prompt=\"$CT\"'\n",
    "\n",
    "\n",
    "if Resume_Training and os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n",
    "  MODELT_NAME = OUTPUT_DIR\n",
    "  print('\u001b[1;32mResuming Training...\u001b[0m')\n",
    "\n",
    "elif Resume_Training and not os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n",
    "  print('\u001b[1;31mPrevious model not found, training a new model...\u001b[0m') \n",
    "  MODELT_NAME = MODEL_NAME\n",
    "  while MODEL_NAME == \"\":\n",
    "    print('\u001b[1;31mNo model found, use the \"Model Download\" cell to download a model.')\n",
    "    time.sleep(5)\n",
    "\n",
    "\n",
    "#@markdown **************************************************** CKPT SAVES ***********************************************************\n",
    "\n",
    "# ****************** Dreambooth absolute path fix **********************\n",
    "#%cd $MODEL_NAME/tokenizer/\n",
    "\n",
    "#search_text_pathfix = '\"add_prefix_space\": false,'\n",
    "#replace_text_pathfix = '\"tokenizer_class\": \"CLIPTokenizer\",\\n  \"add_prefix_space\": false,'\n",
    "\n",
    "#with open(r'tokenizer_config.json', 'r') as file_pathfix:\n",
    "#  data_pathfix = file_pathfix.read()\n",
    "#  data_pathfix = data_pathfix.replace(search_text_pathfix, replace_text_pathfix)\n",
    "\n",
    "#with open(r'tokenizer_config.json', 'w') as file_pathfix:\n",
    "#  file_pathfix.write(data_pathfix)\n",
    "\n",
    "\n",
    "#@markdown **************************************************** ACTIVATE TRAINING ***********************************************************\n",
    "\n",
    "print('\u001b[1;33mTraining...\u001b[0m')\n",
    "!accelerate launch --mixed_precision=$precision /workspace/content/diffusers/train_dreambooth.py \\\n",
    "  --train_text_encoder \\\n",
    "  --save_steps=$Save_Checkpoint_Every \\\n",
    "  --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n",
    "  --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
    "  --class_data_dir=\"$CLASS_DIR\" \\\n",
    "  --output_dir=\"$OUTPUT_DIR\" \\\n",
    "  --instance_prompt=\"$PT\" \\\n",
    "  --with_prior_preservation --prior_loss_weight=1.0 \\\n",
    "  $ClassPromptVar \\\n",
    "  --seed=$Seed \\\n",
    "  --resolution=$Res_Int \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --use_8bit_adam \\\n",
    "  --learning_rate=$Learning_Rate_Def \\\n",
    "  --lr_scheduler=\"polynomial\" \\\n",
    "  --lr_warmup_steps=10 \\\n",
    "  --max_train_steps=$Training_Steps \\\n",
    "  --tokenizer_name=\"$MODELT_NAME/tokenizer/\"\n",
    "\n",
    "\n",
    "if os.path.exists('/workspace/content/models/'+Session_Name+'/unet/diffusion_pytorch_model.bin'):\n",
    "  print(\"Almost done ...\")\n",
    "  %cd /workspace/content\n",
    "\n",
    "  #clear_output()\n",
    "  CKPT_PATH_FINAL = SESSION_DIR + '/' + Session_Name + '.ckpt'\n",
    "  if precision=='fp16':\n",
    "    !python3 /workspace/content/diffusers/convertosd.py --half --model_path $OUTPUT_DIR --checkpoint_path $CKPT_PATH_FINAL\n",
    "  else:\n",
    "    !python3 /workspace/content/diffusers/convertosd.py --model_path $OUTPUT_DIR --checkpoint_path $CKPT_PATH_FINAL\n",
    "  \n",
    "  #clear_output()\n",
    "  if os.path.exists(SESSION_DIR+\"/\"+Session_Name+'.ckpt'):\n",
    "    if not os.path.exists(str(SESSION_DIR+'/tokenizer')):\n",
    "      !cp -R '/workspace/content/models/'$Session_Name'/tokenizer' \"$SESSION_DIR\"\n",
    "    print(\"\u001b[1;32mDONE, the CKPT model is in the sessions folder\")\n",
    "  else:\n",
    "    print(\"\u001b[1;31mSomething went wrong\")\n",
    "    \n",
    "else:\n",
    "  print(\"\u001b[1;31mSomething went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download your new model\n",
    "\n",
    "%cd /workspace/content\n",
    "clear.output()\n",
    "\n",
    "os.symlink(MDLPTH, 'db_checkpoint.ckpt')\n",
    "print(\"Remember! Your activation prompt is:\",PT,CT)\n",
    "FileLink(r'db_checkpoint.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
